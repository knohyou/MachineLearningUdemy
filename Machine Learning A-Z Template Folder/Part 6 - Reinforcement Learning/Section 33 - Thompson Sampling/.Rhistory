dataset = read.csv('Position_Salaries.csv')
dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
setwd("C:/Users/knohy/Projects/Coding/MachineLearning/Udemy/Machine Learning A-Z Template Folder/Part 6 - Reinforcement Learning/Section 32 - Upper Confidence Bound (UCB)")
dataset = read.csv('Ads_CTR_Optimisation.csv')
View(dataset)
dataset = read.csv('Ads_CTR_Optimisation.csv')
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
ads_selected
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implement the UCB
d = 10 # Number of ads
N = 10000 # Total number of iteration
ads_selected = integer(0)
number_of_selections = integer(d)
sums_of_rewards = integer(d)
for (n in 1:N){
max_upper_bound = 0
ad = 0
for (i in 1:d){
if (number_of_selections[i]>0){
average_reward = sums_of_rewards[i]/number_of_selections[i]
delta_i = sqrt(3/2*log(n)/number_of_selections[i])
upper_confidence = average_reward + delta_i
} else {
upper_bound = 1e400
}
if (upper_confidence > max_upper_bound){
max_upper_bound = upper_confidence
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n,ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
}
# Upper confidence Bound
# Import dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implement the UCB
d = 10 # Number of ads
N = 10000 # Total number of iteration
ads_selected = integer(0)
number_of_selections = integer(d)
sums_of_rewards = integer(d)
total_reward = 0
for (n in 1:N){
max_upper_bound = 0
ad = 0
for (i in 1:d){
if (number_of_selections[i]>0){
average_reward = sums_of_rewards[i]/number_of_selections[i]
delta_i = sqrt(3/2*log(n)/number_of_selections[i])
upper_confidence = average_reward + delta_i
} else {
upper_confidence = 1e400
}
if (upper_confidence > max_upper_bound){
max_upper_bound = upper_confidence
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n,ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
number_of_selections
ads_selected
ads_selected
ads_selected[9900:9999]
# Visualize the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of Times each ad selected')
setwd("C:/Users/knohy/Projects/Coding/MachineLearning/Udemy/Machine Learning A-Z Template Folder/Part 6 - Reinforcement Learning/Section 33 - Thompson Sampling")
# Upper confidence Bound
# Import dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implement the UCB
d = 10 # Number of ads
N = 10000 # Total number of iteration
ads_selected = integer(0)
number_of_selections_1 = integer(d)
number_of_selections_0 = integer(d)
total_reward = 0
for (n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
random_beta = rbeta(n = 1,
shape1 = number_of_selections_1[i] + 1,
shape2 = number_of_selections_0[i] + 1)
if (random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n,ad]
if (rewards == 1){
number_of_selections_1[ad] = number_of_selections_1[ad] + 1
}
else {
number_of_selections_0[ad] = number_of_selections_0[ad] + 1
}
total_reward = total_reward + reward
}
# Upper confidence Bound
# Import dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implement the UCB
d = 10 # Number of ads
N = 10000 # Total number of iteration
ads_selected = integer(0)
number_of_selections_1 = integer(d)
number_of_selections_0 = integer(d)
total_reward = 0
for (n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
random_beta = rbeta(n = 1,
shape1 = number_of_selections_1[i] + 1,
shape2 = number_of_selections_0[i] + 1)
if (random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n,ad]
if (reward == 1){
number_of_selections_1[ad] = number_of_selections_1[ad] + 1
}
else {
number_of_selections_0[ad] = number_of_selections_0[ad] + 1
}
total_reward = total_reward + reward
}
# Visualize the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of Times each ad selected')
